[Model(identifier='meta-llama/Llama-3.1-8B-Instruct', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p1-8b-instruct', type='model'), Model(identifier='meta-llama/Llama-3.1-70B-Instruct', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p1-70b-instruct', type='model'), Model(identifier='meta-llama/Llama-3.1-405B-Instruct-FP8', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p1-405b-instruct', type='model'), Model(identifier='meta-llama/Llama-3.2-1B-Instruct', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p2-1b-instruct', type='model'), Model(identifier='meta-llama/Llama-3.2-3B-Instruct', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p2-3b-instruct', type='model'), Model(identifier='meta-llama/Llama-3.2-11B-Vision-Instruct', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p2-11b-vision-instruct', type='model'), Model(identifier='meta-llama/Llama-3.2-90B-Vision-Instruct', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-v3p2-90b-vision-instruct', type='model'), Model(identifier='meta-llama/Llama-Guard-3-8B', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-guard-3-8b', type='model'), Model(identifier='meta-llama/Llama-Guard-3-11B-Vision', metadata={}, provider_id='fireworks', provider_resource_id='fireworks/llama-guard-3-11b-vision', type='model')]
Traceback (most recent call last):
  File "/home/ubuntu/1xa100-2/llama-stack/grammar.py", line 9, in <module>
    response = client.inference.chat_completion(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_utils/_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/resources/inference.py", line 217, in chat_completion
    self._post(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 1261, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 953, in request
    return self._request(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 1041, in _request
    return self._retry_request(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 1090, in _retry_request
    return self._request(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 1041, in _request
    return self._retry_request(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 1090, in _retry_request
    return self._request(
  File "/home/ubuntu/1xa100-2/llama-stack/.venv/lib/python3.10/site-packages/llama_stack_client/_base_client.py", line 1056, in _request
    raise self._make_status_error_from_response(err.response) from None
llama_stack_client.InternalServerError: Error code: 501 - {'detail': 'Not implemented: Grammar response format not supported yet'}
