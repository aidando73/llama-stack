{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import redis\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bank_59'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from counter import get_last_counter\n",
    "\n",
    "bank_id = f\"bank_{get_last_counter()}\"\n",
    "# bank_id = \"bank_52\"\n",
    "bank_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id_by_index', 'chunk_by_index', 'faiss_index'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from json import loads\n",
    "from IPython.display import display\n",
    "\n",
    "index = loads(r.get(f\"faiss_index:v1::{bank_id}\"))\n",
    "# Write index to JSON file\n",
    "import json\n",
    "with open('faiss_index.json', 'w') as f:\n",
    "    json.dump(index, f, indent=2)\n",
    "display(index.keys())\n",
    "display(len(index['id_by_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryDocumentsResponse(chunks=[Chunk(content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRun training on Amazon SageMaker\\n\\n\\n\\n\\n Hugging Face        Models Datasets Spaces Docs  Solutions   Pricing     Log In Sign Up\\n\\n\\n\\n\\nTransformers documentation\\n\\t\\t\\t\\nRun training on Amazon SageMaker\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tTransformers\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nSearch documentation\\n\\n\\nmainv4.34.0v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html\\nDEENESFRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun', document_id='2', token_count=512), Chunk(content=' DEENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on', document_id='44', token_count=512), Chunk(content='ENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models', document_id='14', token_count=512), Chunk(content='ENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models', document_id='28', token_count=512), Chunk(content='ENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models', document_id='49', token_count=512), Chunk(content='1.0v1.0.0doc-builder-html\\nDEENESFRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun inference with multilingual models\\nUse model-specific APIs\\nShare a custom model\\nTemplates for chat models\\nRun training on Amazon SageMaker\\nExport to ONNX\\nExport to TFLite\\nExport to TorchScript\\nBenchmarks\\nNotebooks with examples\\nCommunity resources\\nCustom Tools and Prompts\\nTroubleshoot\\n\\n\\nPerformance and scalability\\n\\n\\nOverview\\n\\nEfficient training techniques\\n\\n\\nMethods and tools for efficient training on a single GPU\\nMultiple GPUs and parallelism\\nEfficient training on CPU\\nDistributed CPU training\\nTraining on TPUs\\nTraining on TPU with TensorFlow\\nTraining on Specialized Hardware\\nCustom hardware for training\\nHyperparameter Search using Trainer API\\n\\n\\nOptimizing inference\\n\\n\\nInference on CPU\\nInference on one GPU\\nInference on many GPUs\\nInference on Specialized Hardware\\n\\nInstantiating a big model\\nTroubleshooting\\nXLA Integration for TensorFlow Models\\nOptimize inference using `torch.compile()`\\n\\n\\nContribute\\n\\n\\nHow to contribute to transformers?\\nHow to add a model to ðŸ¤— Transformers?\\nHow to convert a ðŸ¤— Transformers model to TensorFlow?\\nHow to add a pipeline to ðŸ¤— Transformers?\\nTesting\\nChecks on a Pull Request\\n\\n\\nConceptual guides\\n\\n\\nPhilosophy\\nGlossary\\nWhat ðŸ¤— Transformers can do\\nHow ðŸ¤— Transformers solve tasks\\nThe Transformer model family\\nSummary of the tokenizers\\nAttention mechanisms\\nPadding and truncation\\nBERTology\\nPerplexity of fixed-length models\\nPipelines for webserver inference\\nModel training anatomy\\n\\n\\nAPI\\n\\n\\n\\nMain Classes\\n\\n\\nAgents and Tools\\nAuto Classes\\nCallbacks\\nConfiguration\\nData Collator\\nKeras callbacks\\nLogging\\nModels\\nText Generation\\nONNX\\nOptimization\\nModel outputs\\nPipelines\\nProcessors\\nQuantization\\nTokenizer\\nTrainer\\nDeepSpeed Integration\\nFeature Extractor\\nImage Processor\\n\\n\\nModels', document_id='36', token_count=512), Chunk(content='.0doc-builder-html DEENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n', document_id='31', token_count=512), Chunk(content='FRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models, datasets', document_id='3', token_count=512), Chunk(content='FRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun inference with multilingual models\\nUse model-specific APIs\\nShare a custom model\\nTemplates for chat models\\nRun training on Amazon SageMaker\\nExport to ONNX\\nExport to TFLite\\nExport to TorchScript\\nBenchmarks\\nNotebooks with examples\\nCommunity resources\\nCustom Tools and Prompts\\nTroubleshoot\\n\\n\\nPerformance and scalability\\n\\n\\nOverview\\n\\nEfficient training techniques\\n\\n\\nMethods and tools for efficient training on a single GPU\\nMultiple GPUs and parallelism\\nEfficient training on CPU\\nDistributed CPU training\\nTraining on TPUs\\nTraining on TPU with TensorFlow\\nTraining on Specialized Hardware\\nCustom hardware for training\\nHyperparameter Search using Trainer API\\n\\n\\nOptimizing inference\\n\\n\\nInference on CPU\\nInference on one GPU\\nInference on many GPUs\\nInference on Specialized Hardware\\n\\nInstantiating a big model\\nTroubleshooting\\nXLA Integration for TensorFlow Models\\nOptimize inference using `torch.compile()`\\n\\n\\nContribute\\n\\n\\nHow to contribute to transformers?\\nHow to add a model to ðŸ¤— Transformers?\\nHow to convert a ðŸ¤— Transformers model to TensorFlow?\\nHow to add a pipeline to ðŸ¤— Transformers?\\nTesting\\nChecks on a Pull Request\\n\\n\\nConceptual guides\\n\\n\\nPhilosophy\\nGlossary\\nWhat ðŸ¤— Transformers can do\\nHow ðŸ¤— Transformers solve tasks\\nThe Transformer model family\\nSummary of the tokenizers\\nAttention mechanisms\\nPadding and truncation\\nBERTology\\nPerplexity of fixed-length models\\nPipelines for webserver inference\\nModel training anatomy\\n\\n\\nAPI\\n\\n\\n\\nMain Classes\\n\\n\\nAgents and Tools\\nAuto Classes\\nCallbacks\\nConfiguration\\nData Collator\\nKeras callbacks\\nLogging\\nModels\\nText Generation\\nONNX\\nOptimization\\nModel outputs\\nPipelines\\nProcessors\\nQuantization\\nTokenizer\\nTrainer\\nDeepSpeed Integration\\nFeature Extractor\\nImage Processor\\n\\n\\nModels\\n\\n\\n\\nText models\\n\\n\\nVision models\\n\\n\\nAudio models\\n\\n\\nMultimodal models\\n\\n\\nRe', document_id='4', token_count=512), Chunk(content='FRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun inference with multilingual models\\nUse model-specific APIs\\nShare a custom model\\nTemplates for chat models\\nRun training on Amazon SageMaker\\nExport to ONNX\\nExport to TFLite\\nExport to TorchScript\\nBenchmarks\\nNotebooks with examples\\nCommunity resources\\nCustom Tools and Prompts\\nTroubleshoot\\n\\n\\nPerformance and scalability\\n\\n\\nOverview\\n\\nEfficient training techniques\\n\\n\\nMethods and tools for efficient training on a single GPU\\nMultiple GPUs and parallelism\\nEfficient training on CPU\\nDistributed CPU training\\nTraining on TPUs\\nTraining on TPU with TensorFlow\\nTraining on Specialized Hardware\\nCustom hardware for training\\nHyperparameter Search using Trainer API\\n\\n\\nOptimizing inference\\n\\n\\nInference on CPU\\nInference on one GPU\\nInference on many GPUs\\nInference on Specialized Hardware\\n\\nInstantiating a big model\\nTroubleshooting\\nXLA Integration for TensorFlow Models\\nOptimize inference using `torch.compile()`\\n\\n\\nContribute\\n\\n\\nHow to contribute to transformers?\\nHow to add a model to ðŸ¤— Transformers?\\nHow to convert a ðŸ¤— Transformers model to TensorFlow?\\nHow to add a pipeline to ðŸ¤— Transformers?\\nTesting\\nChecks on a Pull Request\\n\\n\\nConceptual guides\\n\\n\\nPhilosophy\\nGlossary\\nWhat ðŸ¤— Transformers can do\\nHow ðŸ¤— Transformers solve tasks\\nThe Transformer model family\\nSummary of the tokenizers\\nAttention mechanisms\\nPadding and truncation\\nBERTology\\nPerplexity of fixed-length models\\nPipelines for webserver inference\\nModel training anatomy\\n\\n\\nAPI\\n\\n\\n\\nMain Classes\\n\\n\\nAgents and Tools\\nAuto Classes\\nCallbacks\\nConfiguration\\nData Collator\\nKeras callbacks\\nLogging\\nModels\\nText Generation\\nONNX\\nOptimization\\nModel outputs\\nPipelines\\nProcessors\\nQuantization\\nTokenizer\\nTrainer\\nDeepSpeed Integration\\nFeature Extractor\\nImage Processor\\n\\n\\nModels\\n\\n\\n\\nText models\\n\\n\\nALBERT\\nBART\\nBARThez\\nBART', document_id='7', token_count=512)], scores=[1.1615444763272156, 1.0817174566137076, 1.0796262534800076, 1.0796262534800076, 1.0796262534800076, 1.0753352471665694, 1.0694973057646227, 1.0635819447101726, 1.0546415069669437, 1.0546415069669437])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk(content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRun training on Amazon SageMaker\\n\\n\\n\\n\\n Hugging Face        Models Datasets Spaces Docs  Solutions   Pricing     Log In Sign Up\\n\\n\\n\\n\\nTransformers documentation\\n\\t\\t\\t\\nRun training on Amazon SageMaker\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tTransformers\\n\\t\\t\\t\\t\\t\\n\\n\\n\\nSearch documentation\\n\\n\\nmainv4.34.0v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html\\nDEENESFRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun', document_id='2', token_count=512)\n",
      "1.1615444763272156\n",
      "Chunk(content=' DEENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on', document_id='44', token_count=512)\n",
      "1.0817174566137076\n",
      "Chunk(content='ENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models', document_id='14', token_count=512)\n",
      "1.0796262534800076\n",
      "Chunk(content='ENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models', document_id='28', token_count=512)\n",
      "1.0796262534800076\n",
      "Chunk(content='ENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models', document_id='49', token_count=512)\n",
      "1.0796262534800076\n",
      "Chunk(content='1.0v1.0.0doc-builder-html\\nDEENESFRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun inference with multilingual models\\nUse model-specific APIs\\nShare a custom model\\nTemplates for chat models\\nRun training on Amazon SageMaker\\nExport to ONNX\\nExport to TFLite\\nExport to TorchScript\\nBenchmarks\\nNotebooks with examples\\nCommunity resources\\nCustom Tools and Prompts\\nTroubleshoot\\n\\n\\nPerformance and scalability\\n\\n\\nOverview\\n\\nEfficient training techniques\\n\\n\\nMethods and tools for efficient training on a single GPU\\nMultiple GPUs and parallelism\\nEfficient training on CPU\\nDistributed CPU training\\nTraining on TPUs\\nTraining on TPU with TensorFlow\\nTraining on Specialized Hardware\\nCustom hardware for training\\nHyperparameter Search using Trainer API\\n\\n\\nOptimizing inference\\n\\n\\nInference on CPU\\nInference on one GPU\\nInference on many GPUs\\nInference on Specialized Hardware\\n\\nInstantiating a big model\\nTroubleshooting\\nXLA Integration for TensorFlow Models\\nOptimize inference using `torch.compile()`\\n\\n\\nContribute\\n\\n\\nHow to contribute to transformers?\\nHow to add a model to ðŸ¤— Transformers?\\nHow to convert a ðŸ¤— Transformers model to TensorFlow?\\nHow to add a pipeline to ðŸ¤— Transformers?\\nTesting\\nChecks on a Pull Request\\n\\n\\nConceptual guides\\n\\n\\nPhilosophy\\nGlossary\\nWhat ðŸ¤— Transformers can do\\nHow ðŸ¤— Transformers solve tasks\\nThe Transformer model family\\nSummary of the tokenizers\\nAttention mechanisms\\nPadding and truncation\\nBERTology\\nPerplexity of fixed-length models\\nPipelines for webserver inference\\nModel training anatomy\\n\\n\\nAPI\\n\\n\\n\\nMain Classes\\n\\n\\nAgents and Tools\\nAuto Classes\\nCallbacks\\nConfiguration\\nData Collator\\nKeras callbacks\\nLogging\\nModels\\nText Generation\\nONNX\\nOptimization\\nModel outputs\\nPipelines\\nProcessors\\nQuantization\\nTokenizer\\nTrainer\\nDeepSpeed Integration\\nFeature Extractor\\nImage Processor\\n\\n\\nModels', document_id='36', token_count=512)\n",
      "1.0753352471665694\n",
      "Chunk(content='.0doc-builder-html DEENESFRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n', document_id='31', token_count=512)\n",
      "1.0694973057646227\n",
      "Chunk(content='FRITKOPTZH     112,792  Get started  ðŸ¤— Transformers Quick tour Installation  Tutorials  Run inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with ðŸ¤— Accelerate Load and train adapters with ðŸ¤— PEFT Share your model Agents Generation with LLMs  Task Guides  Natural Language Processing  Audio  Computer Vision  Multimodal  Generation  Prompting  Developer guides  Use fast tokenizers from ðŸ¤— Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot  Performance and scalability  Overview Efficient training techniques  Methods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API  Optimizing inference  Inference on CPU Inference on one GPU Inference on many GPUs Inference on Specialized Hardware  Instantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`  Contribute  How to contribute to transformers? How to add a model to ðŸ¤— Transformers? How to convert a ðŸ¤— Transformers model to TensorFlow? How to add a pipeline to ðŸ¤— Transformers? Testing Checks on a Pull Request  Conceptual guides  Philosophy Glossary What ðŸ¤— Transformers can do How ðŸ¤— Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy  API  Main Classes  Agents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor  Models  Text models  Vision models  Audio models  Multimodal models  Reinforcement learning models  Time series models  Graph models  Internal Helpers  Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series   \\n\\n\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\n\\t\\t\\n\\nCollaborate on models, datasets', document_id='3', token_count=512)\n",
      "1.0635819447101726\n",
      "Chunk(content='FRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun inference with multilingual models\\nUse model-specific APIs\\nShare a custom model\\nTemplates for chat models\\nRun training on Amazon SageMaker\\nExport to ONNX\\nExport to TFLite\\nExport to TorchScript\\nBenchmarks\\nNotebooks with examples\\nCommunity resources\\nCustom Tools and Prompts\\nTroubleshoot\\n\\n\\nPerformance and scalability\\n\\n\\nOverview\\n\\nEfficient training techniques\\n\\n\\nMethods and tools for efficient training on a single GPU\\nMultiple GPUs and parallelism\\nEfficient training on CPU\\nDistributed CPU training\\nTraining on TPUs\\nTraining on TPU with TensorFlow\\nTraining on Specialized Hardware\\nCustom hardware for training\\nHyperparameter Search using Trainer API\\n\\n\\nOptimizing inference\\n\\n\\nInference on CPU\\nInference on one GPU\\nInference on many GPUs\\nInference on Specialized Hardware\\n\\nInstantiating a big model\\nTroubleshooting\\nXLA Integration for TensorFlow Models\\nOptimize inference using `torch.compile()`\\n\\n\\nContribute\\n\\n\\nHow to contribute to transformers?\\nHow to add a model to ðŸ¤— Transformers?\\nHow to convert a ðŸ¤— Transformers model to TensorFlow?\\nHow to add a pipeline to ðŸ¤— Transformers?\\nTesting\\nChecks on a Pull Request\\n\\n\\nConceptual guides\\n\\n\\nPhilosophy\\nGlossary\\nWhat ðŸ¤— Transformers can do\\nHow ðŸ¤— Transformers solve tasks\\nThe Transformer model family\\nSummary of the tokenizers\\nAttention mechanisms\\nPadding and truncation\\nBERTology\\nPerplexity of fixed-length models\\nPipelines for webserver inference\\nModel training anatomy\\n\\n\\nAPI\\n\\n\\n\\nMain Classes\\n\\n\\nAgents and Tools\\nAuto Classes\\nCallbacks\\nConfiguration\\nData Collator\\nKeras callbacks\\nLogging\\nModels\\nText Generation\\nONNX\\nOptimization\\nModel outputs\\nPipelines\\nProcessors\\nQuantization\\nTokenizer\\nTrainer\\nDeepSpeed Integration\\nFeature Extractor\\nImage Processor\\n\\n\\nModels\\n\\n\\n\\nText models\\n\\n\\nVision models\\n\\n\\nAudio models\\n\\n\\nMultimodal models\\n\\n\\nRe', document_id='4', token_count=512)\n",
      "1.0546415069669437\n",
      "Chunk(content='FRITKOPTZH\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\n\\nðŸ¤— Transformers\\nQuick tour\\nInstallation\\n\\n\\nTutorials\\n\\n\\nRun inference with pipelines\\nWrite portable code with AutoClass\\nPreprocess data\\nFine-tune a pretrained model\\nTrain with a script\\nSet up distributed training with ðŸ¤— Accelerate\\nLoad and train adapters with ðŸ¤— PEFT\\nShare your model\\nAgents\\nGeneration with LLMs\\n\\n\\nTask Guides\\n\\n\\n\\nNatural Language Processing\\n\\n\\nAudio\\n\\n\\nComputer Vision\\n\\n\\nMultimodal\\n\\n\\nGeneration\\n\\n\\nPrompting\\n\\n\\n\\nDeveloper guides\\n\\n\\nUse fast tokenizers from ðŸ¤— Tokenizers\\nRun inference with multilingual models\\nUse model-specific APIs\\nShare a custom model\\nTemplates for chat models\\nRun training on Amazon SageMaker\\nExport to ONNX\\nExport to TFLite\\nExport to TorchScript\\nBenchmarks\\nNotebooks with examples\\nCommunity resources\\nCustom Tools and Prompts\\nTroubleshoot\\n\\n\\nPerformance and scalability\\n\\n\\nOverview\\n\\nEfficient training techniques\\n\\n\\nMethods and tools for efficient training on a single GPU\\nMultiple GPUs and parallelism\\nEfficient training on CPU\\nDistributed CPU training\\nTraining on TPUs\\nTraining on TPU with TensorFlow\\nTraining on Specialized Hardware\\nCustom hardware for training\\nHyperparameter Search using Trainer API\\n\\n\\nOptimizing inference\\n\\n\\nInference on CPU\\nInference on one GPU\\nInference on many GPUs\\nInference on Specialized Hardware\\n\\nInstantiating a big model\\nTroubleshooting\\nXLA Integration for TensorFlow Models\\nOptimize inference using `torch.compile()`\\n\\n\\nContribute\\n\\n\\nHow to contribute to transformers?\\nHow to add a model to ðŸ¤— Transformers?\\nHow to convert a ðŸ¤— Transformers model to TensorFlow?\\nHow to add a pipeline to ðŸ¤— Transformers?\\nTesting\\nChecks on a Pull Request\\n\\n\\nConceptual guides\\n\\n\\nPhilosophy\\nGlossary\\nWhat ðŸ¤— Transformers can do\\nHow ðŸ¤— Transformers solve tasks\\nThe Transformer model family\\nSummary of the tokenizers\\nAttention mechanisms\\nPadding and truncation\\nBERTology\\nPerplexity of fixed-length models\\nPipelines for webserver inference\\nModel training anatomy\\n\\n\\nAPI\\n\\n\\n\\nMain Classes\\n\\n\\nAgents and Tools\\nAuto Classes\\nCallbacks\\nConfiguration\\nData Collator\\nKeras callbacks\\nLogging\\nModels\\nText Generation\\nONNX\\nOptimization\\nModel outputs\\nPipelines\\nProcessors\\nQuantization\\nTokenizer\\nTrainer\\nDeepSpeed Integration\\nFeature Extractor\\nImage Processor\\n\\n\\nModels\\n\\n\\n\\nText models\\n\\n\\nALBERT\\nBART\\nBARThez\\nBART', document_id='7', token_count=512)\n",
      "1.0546415069669437\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Test run\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:5001\",\n",
    ")\n",
    "\n",
    "response = client.memory.query(\n",
    "    bank_id=bank_id,\n",
    "    query=[\"Run training on Amazon SageMaker\"],\n",
    "    params={\"max_chunks\": 10},\n",
    ")\n",
    "\n",
    "display(response)\n",
    "\n",
    "for chunk, score in zip(response.chunks, response.scores):\n",
    "    print(chunk)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>178385.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>92226.00916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>253.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>159669.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>209115.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>240418.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296460.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       context_length\n",
       "count        50.00000\n",
       "mean     178385.28000\n",
       "std       92226.00916\n",
       "min         253.00000\n",
       "25%      159669.25000\n",
       "50%      209115.50000\n",
       "75%      240418.50000\n",
       "max      296460.00000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "# dataset = datasets.load_dataset(path=\"data/transformers.parquet\")\n",
    "\n",
    "# df = dataset[\"train\"].to_pandas()\n",
    "df = pd.read_parquet(\"data/transformers.parquet\")\n",
    "df = df.rename(columns={'html': 'context'})\n",
    "df['context_length'] = df['context'].apply(lambda x: len(x))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Translation',\n",
       " 'Run training on Amazon SageMaker',\n",
       " 'Create a custom architecture',\n",
       " 'Image Processor']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids = []\n",
    "titles = []\n",
    "for i, row in df.iterrows():\n",
    "    titles.append(row[\"title\"])\n",
    "    doc_ids.append(i)\n",
    "\n",
    "print(len(titles))\n",
    "print(len(doc_ids))\n",
    "display(titles[:5])\n",
    "doc_ids[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                              | 9/50 [00:00<00:00, 88.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 93.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [24, 1, 28, 1, 1, 24, 1, 41, 22, 35],\n",
       " [2, 44, 14, 28, 49, 36, 31, 3, 4, 7],\n",
       " [3, 3, 28, 8, 16, 18, 23, 24, 2, 48],\n",
       " [4, 35, 26, 42, 18, 48, 35, 40, 24, 3],\n",
       " [5, 5, 5, 5, 5, 2, 31, 33, 1, 26],\n",
       " [6, 24, 6, 41, 6, 36, 24, 6, 5, 17],\n",
       " [7, 7, 7, 7, 7, 17, 5, 17, 17, 33],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 20],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [10, 10, 10, 10, 15, 41, 31, 32, 26, 5],\n",
       " [11, 11, 33, 11, 26, 41, 45, 22, 33, 17],\n",
       " [6, 12, 12, 12, 28, 12, 18, 28, 28, 28],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [26, 14, 41, 7, 5, 26, 14, 14, 14, 14],\n",
       " [26, 33, 35, 47, 11, 43, 45, 40, 3, 1],\n",
       " [16, 16, 16, 16, 16, 16, 19, 42, 16, 16],\n",
       " [17, 17, 7, 7, 17, 7, 7, 41, 26, 5],\n",
       " [48, 18, 3, 44, 8, 38, 41, 4, 7, 15],\n",
       " [19, 19, 19, 19, 19, 19, 47, 28, 40, 7],\n",
       " [20, 20, 43, 3, 6, 15, 24, 14, 4, 36],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [49, 49, 41, 32, 1, 22, 22, 24, 22, 32],\n",
       " [23, 23, 36, 36, 1, 15, 23, 15, 22, 36],\n",
       " [24, 6, 41, 24, 6, 5, 17, 16, 36, 7],\n",
       " [24, 25, 25, 25, 25, 25, 16, 11, 42, 39],\n",
       " [26, 7, 12, 32, 8, 14, 1, 28, 22, 15],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [28, 28, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       " [29, 29, 29, 29, 29, 29, 39, 29, 40, 11],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [31, 31, 31, 31, 31, 31, 5, 24, 10, 38],\n",
       " [24, 32, 32, 1, 39, 32, 41, 1, 39, 22],\n",
       " [33, 33, 5, 33, 32, 5, 33, 12, 26, 26],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [35, 35, 35, 35, 35, 35, 35, 42, 16, 40],\n",
       " [36, 36, 36, 36, 36, 41, 23, 5, 17, 24],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [25, 16, 18, 23, 3, 44, 48, 2, 40, 24],\n",
       " [39, 39, 39, 39, 32, 24, 41, 26, 35, 1],\n",
       " [40, 47, 40, 35, 42, 47, 40, 19, 24, 19],\n",
       " [41, 41, 41, 41, 41, 17, 17, 14, 16, 24],\n",
       " [42, 42, 42, 42, 42, 42, 42, 35, 40, 11],\n",
       " [43, 43, 43, 20, 12, 24, 4, 48, 15, 44],\n",
       " [38, 44, 44, 44, 44, 44, 44, 38, 38, 44],\n",
       " [45, 45, 45, 45, 45, 35, 11, 45, 40, 19],\n",
       " [44, 26, 41, 22, 28, 47, 25, 6, 17, 12],\n",
       " [47, 47, 42, 47, 47, 35, 19, 40, 26, 45],\n",
       " [48, 44, 8, 38, 41, 44, 44, 48, 3, 4],\n",
       " [49, 49, 49, 49, 8, 33, 49, 41, 3, 40]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, len(titles))):\n",
    "    response = client.memory.query(\n",
    "        bank_id=bank_id,\n",
    "        query=[titles[i]],\n",
    "        params={\"max_chunks\": top_k},\n",
    "    )\n",
    "    res = [int(chunk.document_id) for chunk in response.chunks]\n",
    "    results.append(res)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 0., 1., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 0., 0., 1.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "res = np.array(results)\n",
    "\n",
    "\n",
    "targets = np.array(doc_ids)\n",
    "display(targets.shape)\n",
    "targets = np.expand_dims(targets, axis=1)\n",
    "display(targets.shape)\n",
    "targets = targets == res\n",
    "display(targets.shape)\n",
    "targets = targets.astype(float)\n",
    "display(targets)\n",
    "\n",
    "targets = targets[:, :top_k]\n",
    "display(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5666666666666667)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.sum(targets, axis=1)\n",
    "temp = np.divide(temp, top_k)\n",
    "precision = np.sum(temp) / len(temp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.76)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.logical_or.reduce(targets, axis=1)\n",
    "temp = temp.astype(float)\n",
    "recall = np.sum(temp) / len(temp)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.69)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = np.tile(np.arange(1, top_k + 1), (len(titles), 1)).astype(float)\n",
    "temp = np.copy(targets)\n",
    "for i in range(len(temp)):\n",
    "    first_true = True\n",
    "    for j in range(len(temp[i])):\n",
    "        if temp[i,j]:\n",
    "            if first_true:\n",
    "                first_true = False\n",
    "            else:\n",
    "                temp[i,j] = False\n",
    "reciprocal_ranks = temp / ranks\n",
    "reciprocal_ranks = np.sum(reciprocal_ranks) / len(temp)\n",
    "reciprocal_ranks = np.mean(reciprocal_ranks)\n",
    "reciprocal_ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
