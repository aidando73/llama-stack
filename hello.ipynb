{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(completion_message=CompletionMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", role='assistant', stop_reason='end_of_turn', tool_calls=[]), logprobs=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from IPython.display import display\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:5001\",\n",
    ")\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world client!\"},\n",
    "    ],\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "display(response)\n",
    "\n",
    "# for chunk in response:\n",
    "    # print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inference': [ProviderInfo(provider_id='groq', provider_type='remote::groq')],\n",
       " 'memory': [ProviderInfo(provider_id='faiss', provider_type='inline::faiss')],\n",
       " 'safety': [ProviderInfo(provider_id='llama-guard', provider_type='inline::llama-guard')],\n",
       " 'agents': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'telemetry': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'eval': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'datasetio': [ProviderInfo(provider_id='localfs', provider_type='inline::localfs')],\n",
       " 'scoring': [ProviderInfo(provider_id='basic', provider_type='inline::basic')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.providers.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is ASGI?**\n",
      "\n",
      "ASGI (Asynchronous Server Gateway Interface) is a standard for building asynchronous web servers and frameworks in Python. It was designed to replace the traditional WSGI (Web Server Gateway Interface) standard, which is synchronous and not suitable for modern asynchronous web development.\n",
      "\n",
      "**Key Features of ASGI**\n",
      "\n",
      "1. **Asynchronous**: ASGI allows for asynchronous execution of web requests, which means that the server can handle multiple requests concurrently without blocking.\n",
      "2. **Event-driven**: ASGI uses an event-driven approach, where the server waits for events (e.g., incoming requests) and then responds to them.\n",
      "3. **Non-blocking**: ASGI allows the server to continue executing other tasks while waiting for an event, making it more efficient and scalable.\n",
      "\n",
      "**How ASGI Works in Python**\n",
      "\n",
      "Here's a high-level overview of how ASGI works in Python:\n",
      "\n",
      "1. **ASGI Application**: An ASGI application is a Python function that takes an `app` object as an argument. The `app` object represents the ASGI server and provides methods for handling requests and responses.\n",
      "2. **Request Handling**: When a request is received, the ASGI server creates an `ASGIRequest` object, which contains information about the request, such as the URL, headers, and body.\n",
      "3. **Middleware**: ASGI applications can use middleware functions to modify or extend the request and response objects. Middleware functions are executed before and after the ASGI application handles the request.\n",
      "4. **ASGI Application Execution**: The ASGI application is executed, which may involve calling other functions or methods to handle the request.\n",
      "5. **Response Generation**: The ASGI application generates a response, which is an `ASGIResponse` object that contains information about the response, such as the status code, headers, and body.\n",
      "6. **Response Sending**: The ASGI server sends the response back to the client.\n",
      "\n",
      "**ASGI Frameworks in Python**\n",
      "\n",
      "Some popular ASGI frameworks in Python include:\n",
      "\n",
      "1. **Sanic**: A modern, asynchronous web framework that provides a simple and efficient way to build web applications.\n",
      "2. **Aiohttp**: A popular asynchronous HTTP client and server library that provides a simple and efficient way to build web applications.\n",
      "3. **FastAPI**: A modern, asynchronous web framework that provides a simple and efficient way to build web applications with high performance and scalability.\n",
      "\n",
      "**Example Code**\n",
      "\n",
      "Here's an example of a simple ASGI application using the Sanic framework:\n",
      "```python\n",
      "from sanic import Sanic\n",
      "\n",
      "app = Sanic()\n",
      "\n",
      "@app.route('/')\n",
      "async def index(request):\n",
      "    return 'Hello, World!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "```\n",
      "This code defines a simple ASGI application that responds to GET requests to the root URL ('/') with the string 'Hello, World!'.\n",
      "\n",
      "I hope this helps you understand how ASGI works in Python! Let me know if you have any questions or need further clarification."
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is ASGI?**\n",
      "\n",
      "ASGI (Asynchronous Server Gateway Interface) is a standard for building asynchronous web servers and frameworks in Python. It was designed to replace the traditional WSGI (Web Server Gateway Interface) standard, which is synchronous and not suitable for modern asynchronous web development.\n",
      "\n",
      "**Key Features of ASGI**\n",
      "\n",
      "1. **Asynchronous**: ASGI allows for asynchronous execution of web requests, which means that the server can handle multiple requests concurrently without blocking.\n",
      "2. **Event-driven**: ASGI uses an event-driven approach, where the server waits for events (e.g., incoming requests) and then responds to them.\n",
      "3. **Non-blocking**: ASGI allows the server to continue executing other tasks while waiting for an event, making it more efficient and scalable.\n",
      "\n",
      "**How ASGI Works in Python**\n",
      "\n",
      "Here's a high-level overview of how ASGI works in Python:\n",
      "\n",
      "1. **ASGI Application**: An ASGI application is a Python function that takes an `app` object as an argument. The `app` object represents the ASGI server and provides methods for handling requests and responses.\n",
      "2. **Request Handling**: When a request is received, the ASGI server creates an `ASGIRequest` object, which contains information about the request, such as the URL, headers, and body.\n",
      "3. **Middleware**: ASGI applications can use middleware functions to modify or extend the request and response objects. Middleware functions are executed before and after the ASGI application handles the request.\n",
      "4. **ASGI Application Execution**: The ASGI application is executed, which may involve calling other functions or methods to handle the request.\n",
      "5. **Response Generation**: The ASGI application generates a response, which is an `ASGIResponse` object that contains information about the response, such as the status code, headers, and body.\n",
      "6. **Response Sending**: The ASGI server sends the response back to the client.\n",
      "\n",
      "**ASGI Frameworks in Python**\n",
      "\n",
      "Some popular ASGI frameworks in Python include:\n",
      "\n",
      "1. **Sanic**: A modern, asynchronous web framework that provides a simple and efficient way to build web applications.\n",
      "2. **Aiohttp**: A popular asynchronous HTTP client and server library that provides a simple and efficient way to build web applications.\n",
      "3. **FastAPI**: A modern, asynchronous web framework that provides a simple and efficient way to build web applications with high performance and scalability.\n",
      "\n",
      "**Example Code**\n",
      "\n",
      "Here's an example of a simple ASGI application using the Sanic framework:\n",
      "```python\n",
      "from sanic import Sanic\n",
      "\n",
      "app = Sanic()\n",
      "\n",
      "@app.route('/')\n",
      "async def index(request):\n",
      "    return 'Hello, World!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "```\n",
      "This code defines a simple ASGI application that responds to GET requests to the root URL ('/') with the string 'Hello, World!'.\n",
      "\n",
      "I hope this helps you understand how ASGI works in Python! Let me know if you have any questions or need further clarification."
     ]
    }
   ],
   "source": [
    "from llama_models.datatypes import SamplingParams\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is ASGI?**\n",
      "\n",
      "ASGI (Asynchronous Server Gateway Interface) is a standard for building asynchronous web servers and frameworks in Python. It was designed to replace the traditional WSGI (Web Server Gateway Interface) standard, which is synchronous and not suitable for modern asynchronous web development.\n",
      "\n",
      "**Key Features of ASGI**\n",
      "\n",
      "1. **Asynchronous**: ASGI allows for asynchronous execution of web requests, which means that the server can handle multiple requests concurrently without blocking.\n",
      "2. **Event-driven**: ASGI uses an event-driven approach, where the server waits for events (e.g., incoming requests) and then responds to them.\n",
      "3. **Non-blocking**: ASGI allows the server to continue executing other tasks while waiting for an event, making it more efficient and scalable.\n",
      "\n",
      "**How ASGI Works in Python**\n",
      "\n",
      "Here's a high-level overview of how ASGI works in Python:\n",
      "\n",
      "1. **ASGI Application**: An ASGI application is a Python function that takes an `app` object as an argument. The `app` object represents the ASGI server and provides methods for handling requests and responses.\n",
      "2. **Request Handling**: When a request is received, the ASGI server creates an `ASGIRequest` object, which contains information about the request, such as the URL, headers, and body.\n",
      "3. **Middleware**: ASGI applications can use middleware functions to modify or extend the request and response objects. Middleware functions are executed before and after the ASGI application handles the request.\n",
      "4. **ASGI Application Execution**: The ASGI application is executed, which may involve calling other functions or methods to handle the request.\n",
      "5. **Response Generation**: The ASGI application generates a response, which is an `ASGIResponse` object that contains information about the response, such as the status code, headers, and body.\n",
      "6. **Response Sending**: The ASGI server sends the response back to the client.\n",
      "\n",
      "**ASGI Frameworks in Python**\n",
      "\n",
      "Some popular ASGI frameworks in Python include:\n",
      "\n",
      "1. **Sanic**: A modern, asynchronous web framework that provides a simple and efficient way to build web applications.\n",
      "2. **Aiohttp**: A popular asynchronous HTTP client and server library that provides a simple and efficient way to build web applications.\n",
      "3. **FastAPI**: A modern, asynchronous web framework that provides a simple and efficient way to build web applications with high performance and scalability.\n",
      "\n",
      "**Example Code**\n",
      "\n",
      "Here's an example of a simple ASGI application using the Sanic framework:\n",
      "```python\n",
      "from sanic import Sanic\n",
      "\n",
      "app = Sanic()\n",
      "\n",
      "@app.route('/')\n",
      "async def index(request):\n",
      "    return 'Hello, World!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run()\n",
      "```\n",
      "This code defines a simple ASGI application that responds to GET requests to the root URL ('/') with the string 'Hello, World!'.\n",
      "\n",
      "I hope this helps you understand how ASGI works in Python! Let me know if you have any questions or need further clarification."
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        top_p=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is ASGI?**\n",
      "\n",
      "ASGI (Asynchronous Server Gateway Interface) is a standard for building asynchronous web servers and frameworks in Python. It was designed to replace the traditional WSGI (Web Server Gateway Interface) standard, which is synchronous"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is ASGI?**\n",
      "\n",
      "ASGI (Asynchronous Server Gateway Interface) is a standard for building asynchronous web servers and frameworks in Python. It was designed to replace the traditional WSGI (Web Server Gateway Interface) standard, which is synchronous"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    "    logprobs={\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(completion_message=CompletionMessage(content='', role='assistant', stop_reason='end_of_message', tool_calls=[ToolCall(arguments={'origin': 'ADL', 'destination': 'SYD'}, call_id='call_v514', tool_name='get_flight_info')]), logprobs=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "    ],\n",
    "    # stream=True,\n",
    "    tools=[\n",
    "        {\n",
    "            \"tool_name\": \"get_flight_info\",\n",
    "            \"description\": \"Get the flight information for a given origin and destination\",\n",
    "            \"parameters\": {\n",
    "                \"origin\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The origin airport code. E.g., AU\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk.event.delta, end='')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ChatCompletion(\n",
    "    id='chatcmpl-7f14606b-d091-4b12-9d13-e95831f04301',\n",
    "    choices=[\n",
    "        Choice(\n",
    "            finish_reason='tool_calls',\n",
    "            index=0,\n",
    "            logprobs=None,\n",
    "            message=ChatCompletionMessage(\n",
    "                content=None,\n",
    "                role='assistant',\n",
    "                function_call=None,\n",
    "                tool_calls=[ChatCompletionMessageToolCall(id='call_4qg1', function=Function(arguments='{\"origin\":\"ADL\",\"destination\":\"SYD\"}', name='get_flight_info'), type='function')]\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    created=1733917567,\n",
    "    model='llama3-8b-8192',\n",
    "    object='chat.completion',\n",
    "    system_fingerprint='fp_a97cfe35ae',\n",
    "    usage=CompletionUsage(completion_tokens=76, prompt_tokens=972, total_tokens=1048, completion_time=0.063333333, prompt_time=0.11611327, queue_time=0.0061331509999999895, total_time=0.179446603),\n",
    "    x_groq={'id': 'req_01jetrmtcmfs89v7qyw8fdx1v0'}\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatCompletionResponse' object has no attribute 'event'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/pydantic/main.py:884\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpydantic_extra\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'event'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39minference\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m      2\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241m.\u001b[39mdelta, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/pydantic/main.py:886\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pydantic_extra[item]\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 886\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, item):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChatCompletionResponse' object has no attribute 'event'"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    tools=[\n",
    "        {\n",
    "            \"tool_name\": \"get_flight_info\",\n",
    "            \"description\": \"Get the flight information for a given origin and destination\",\n",
    "            \"parameters\": {\n",
    "                \"origin\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The origin airport code. E.g., AU\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ChatCompletionChunk(\n",
    "    id='chatcmpl-189b0530-6bcb-4089-bad7-65f73104b182', \n",
    "    choices=[\n",
    "        Choice(\n",
    "            delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), \n",
    "            finish_reason=None, \n",
    "            index=0, \n",
    "            logprobs=None\n",
    "        )\n",
    "    ], \n",
    "    created=1733955177, \n",
    "    model='llama3-8b-8192', \n",
    "    object='chat.completion.chunk', \n",
    "    system_fingerprint='fp_a97cfe35ae', \n",
    "    usage=None, \n",
    "    x_groq=XGroq(id='req_01jevwgjx2f3maj4rbzaaexagx', usage=None, error=None))\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
