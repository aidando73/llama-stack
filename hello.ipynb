{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(completion_message=CompletionMessage(content=\"Hello there! How are you today? I'm excited to be your AI companion. What brings you to our conversation today? Do you have a specific topic you'd like to discuss or ask about? I'm all ears!\", role='assistant', stop_reason='end_of_turn', tool_calls=[]), logprobs=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from IPython.display import display\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:5001\",\n",
    ")\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world client!\"},\n",
    "    ],\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "display(response)\n",
    "\n",
    "# for chunk in response:\n",
    "    # print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inference': [ProviderInfo(provider_id='groq', provider_type='remote::groq')],\n",
       " 'memory': [ProviderInfo(provider_id='faiss', provider_type='inline::faiss')],\n",
       " 'safety': [ProviderInfo(provider_id='llama-guard', provider_type='inline::llama-guard')],\n",
       " 'agents': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'telemetry': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'eval': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'datasetio': [ProviderInfo(provider_id='localfs', provider_type='inline::localfs')],\n",
       " 'scoring': [ProviderInfo(provider_id='basic', provider_type='inline::basic')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.providers.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a Python specification for server-side APIs. It allows developers to create asynchronous, concurrent, and scalable web applications. ASGI is designed to be a successor to the WSGI (Web Server Gateway Interface) standard, which is used for traditional synchronous web applications.\n",
      "\n",
      "Here's how ASGI works:\n",
      "\n",
      "**ASGI Components**\n",
      "\n",
      "1. **ASGI Application**: The ASGI application is the main entry point for the ASGI server. It's responsible for handling requests and sending responses. ASGI applications are similar to WSGI applications, but they're designed to handle asynchronous requests and responses.\n",
      "2. **ASGI Server**: The ASGI server is responsible for handling incoming requests and forwarding them to the ASGI application. ASGI servers can be implemented using various technologies, such as asyncio, uvloop, or traditional threading.\n",
      "3. **ASGI Protocol**: The ASGI protocol defines the communication between the ASGI application and the ASGI server. It specifies how requests and responses are serialized and transmitted over the network.\n",
      "\n",
      "**ASGI Request and Response**\n",
      "\n",
      "1. **Request**: An ASGI request consists of two parts:\n",
      "\t* **Message**: The message is the underlying data that represents the request. It includes information such as the request method, URI, headers, and body.\n",
      "\t* **Body**: The body is the actual data sent in the request, such as JSON or form data.\n",
      "2. **Response**: An ASGI response consists of two parts:\n",
      "\t* **Message**: The message is the response to the request. It includes information such as the response status code, headers, and body.\n",
      "\t* **Body**: The body is the actual data sent in the response, such as JSON or HTML.\n",
      "\n",
      "**ASGI Protocol Messages**\n",
      "\n",
      "The ASGI protocol uses a text-based protocol to communicate between the ASGI application and the ASGI server. There are three types of messages:\n",
      "\n",
      "1. **onCONNECT**: Sent from the ASGI application to the ASGI server when a new connection is established.\n",
      "2. **onSEND**: Sent from the ASGI application to the ASGI server to transmit request data.\n",
      "3. **onRECV**: Sent from the ASGI server to the ASGI application when request data is received.\n",
      "4. **onSHUTDOWN**: Sent from the ASGI application to the ASGI server when the connection is closed.\n",
      "\n",
      "**How ASGI Works**\n",
      "\n",
      "Here's an example of how ASGI works:\n",
      "\n",
      "1. A client sends a request to the ASGI server.\n",
      "2. The ASGI server creates a new connection and sends an `onCONNECT` message to the ASGI application.\n",
      "3. The ASGI application receives the `onCONNECT` message and initializes a new request.\n",
      "4. The ASGI application sends an `onSEND` message to the ASGI server to transmit the request data.\n",
      "5. The ASGI server receives the request data and sends an `onRECV` message to the ASGI application.\n",
      "6. The ASGI application processes the request and generates a response.\n",
      "7. The ASGI application sends an `onSEND` message to the ASGI server to transmit the response data.\n",
      "8. The ASGI server receives the response data and sends it back to the client.\n",
      "\n",
      "**Benefits of ASGI**\n",
      "\n",
      "1. **Concurrency**: ASGI allows for true concurrency, enabling multiple requests to be handled simultaneously.\n",
      "2. **Scalability**: ASGI applications can be scaled vertically or horizontally to handle increasing traffic.\n",
      "3. **Low-overhead**: ASGI applications have lower overhead compared to traditional synchronous web applications.\n",
      "4. **Flexibility**: ASGI applications can be implemented using various programming languages and frameworks.\n",
      "\n",
      "In summary, ASGI is a Python specification that enables asynchronous, concurrent, and scalable web applications. It provides a lightweight and flexible framework for building high-performance web applications."
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a specification for building asynchronous web servers and applications in Python. It provides a standardized way for developers to write asynchronous code that can be executed by multiple frameworks and servers.\n",
      "\n",
      "Here's a high-level overview of how ASGI works:\n",
      "\n",
      "**ASGI Spec**\n",
      "\n",
      "The ASGI specification defines a few key components:\n",
      "\n",
      "1. **ASGI Application**: This is the core of the ASGI system. It's an object that implements the `asgi` module, which provides an interface for handling incoming requests and sending responses.\n",
      "2. **ASGI Server**: This is the component that runs the ASGI application. It's responsible for accepting incoming connections, dispatching requests to the application, and sending responses back to the client.\n",
      "3. **ASGI Protocol**: This is the communication protocol used by the ASGI server and application to exchange messages. It's based on TCP and supports bidirectional communication.\n",
      "\n",
      "**ASGI Request/Response Cycle**\n",
      "\n",
      "Here's a step-by-step breakdown of the ASGI request/response cycle:\n",
      "\n",
      "1. **Client connects**: A client (e.g., a web browser) establishes a connection to the ASGI server.\n",
      "2. **Server dispatches**: The server dispatches the incoming connection to the ASGI application.\n",
      "3. **Application receives**: The ASGI application receives the request from the server and performs any necessary processing (e.g., handling forms, authenticating users, or validating input).\n",
      "4. **Application sends response**: The ASGI application sends a response back to the server, which is then sent to the client.\n",
      "5. **Server sends response**: The server sends the response to the client.\n",
      "\n",
      "**ASGI Protocol Messages**\n",
      "\n",
      "The ASGI protocol uses the following types of messages to communicate between the server and application:\n",
      "\n",
      "1. ** message**: This is the primary message type used for request/response exchange. It contains the request data, such as headers, method, and path.\n",
      "2. **ready**: This message is used to indicate that the application is ready to receive the next message.\n",
      "3. **complete**: This message is used to indicate that the response is complete and can be sent to the client.\n",
      "\n",
      "**Python Framework Support**\n",
      "\n",
      "Several Python frameworks support ASGI, including:\n",
      "\n",
      "1. **ASGI**: The official ASGI specification provides a reference implementation in Python.\n",
      "2. **Django**: Django 3.1 and later versions support ASGI through the `django.asgi` module.\n",
      "3. **FastAPI**: FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "4. **Sanic**: Sanic is a Python 3.7+ web framework that's built on top of uvloop and httptools, allowing for high-performance async web development.\n",
      "\n",
      "When using ASGI with a Python framework, you typically need to:\n",
      "\n",
      "1. Create an ASGI application object, which implements the `asgi` module.\n",
      "2. Register the application with the ASGI server.\n",
      "3. Start the ASGI server to begin handling incoming requests and sending responses.\n",
      "\n",
      "By using ASGI, you can build high-performance, scalable, and concurrent web applications with Python."
     ]
    }
   ],
   "source": [
    "from llama_models.datatypes import SamplingParams\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a standard for Python web servers that allows them to communicate with web frameworks and other applications in an asynchronous manner. It's designed to be a replacement for the older WSGI (Web Server Gateway Interface) standard, which was synchronous.\n",
      "\n",
      "Here's a high-level overview of how ASGI works:\n",
      "\n",
      "**The Problem with WSGI**\n",
      "\n",
      "WSGI was introduced in the early 2000s as a way for web frameworks to communicate with web servers. However, WSGI is synchronous, meaning that it blocks the execution of the web server until the web framework has finished processing the request. This can lead to performance issues and scalability problems, especially in modern web applications that require handling many concurrent requests.\n",
      "\n",
      "**The Solution: ASGI**\n",
      "\n",
      "ASGI addresses the limitations of WSGI by introducing an asynchronous programming model. Instead of blocking the execution of the web server, ASGI allows the web framework to yield control back to the web server at specific points in the request processing pipeline. This allows the web server to handle other requests concurrently, improving performance and scalability.\n",
      "\n",
      "**The ASGI Protocol**\n",
      "\n",
      "The ASGI protocol defines a set of messages that can be sent between the web server and the web framework. These messages include:\n",
      "\n",
      "1. `connect`: The web server sends a `connect` message to the web framework to initiate a new connection.\n",
      "2. `receive`: The web framework sends a `receive` message to the web server to receive a request.\n",
      "3. `send`: The web framework sends a `send` message to the web server to send a response.\n",
      "4. `close`: The web framework sends a `close` message to the web server to close the connection.\n",
      "\n",
      "**How ASGI Works**\n",
      "\n",
      "Here's a step-by-step explanation of how ASGI works:\n",
      "\n",
      "1. The web server receives a request and sends a `connect` message to the web framework.\n",
      "2. The web framework processes the request and yields control back to the web server at specific points in the request processing pipeline (e.g., when waiting for I/O operations to complete).\n",
      "3. The web server handles other requests concurrently while the web framework is yielding control.\n",
      "4. When the web framework is ready to send a response, it sends a `send` message to the web server.\n",
      "5. The web server sends the response back to the client.\n",
      "6. When the response is complete, the web framework sends a `close` message to the web server to close the connection.\n",
      "\n",
      "**ASGI Implementations**\n",
      "\n",
      "There are several ASGI implementations available for Python, including:\n",
      "\n",
      "1. `uvicorn`: A popular ASGI server that supports HTTP/1.1 and HTTP/2.\n",
      "2. `hypercorn`: Another popular ASGI server that supports HTTP/1.1 and HTTP/2.\n",
      "3. `daphne`: An ASGI server that supports WebSockets and other advanced features.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "ASGI is a powerful standard for Python web servers that allows them to communicate with web frameworks in an asynchronous manner. By yielding control back to the web server at specific points in the request processing pipeline, ASGI enables web servers to handle many concurrent requests, improving performance and scalability."
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        top_p=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a standard for Python web servers that allows them to communicate with web frameworks and other applications in an asynchronous manner. It's designed to be a replacement for the older WSGI (Web Server Gateway Interface"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ToolDefinition' from 'llama_models.datatypes' (/Users/aidand/dev/llama-stack/envs/lib/python3.10/site-packages/llama_models/datatypes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatatypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolDefinition\n\u001b[1;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39minference\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m      4\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToolDefinition' from 'llama_models.datatypes' (/Users/aidand/dev/llama-stack/envs/lib/python3.10/site-packages/llama_models/datatypes.py)"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    "    logprobs={\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RemoteProtocolError",
     "evalue": "peer closed connection without sending complete message body (incomplete chunked read)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_transports/default.py:127\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_sync/http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_sync/http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_sync/http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_sync/http11.py:213\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions({h11\u001b[38;5;241m.\u001b[39mRemoteProtocolError: RemoteProtocolError}):\n\u001b[1;32m    214\u001b[0m         event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: peer closed connection without sending complete message body (incomplete chunked read)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39minference\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m      2\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mdelta, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/llama_stack_client/_streaming.py:45\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[_T]:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/llama_stack_client/_streaming.py:57\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m process_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_process_response_data\n\u001b[1;32m     55\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_events()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m process_data(data\u001b[38;5;241m=\u001b[39msse\u001b[38;5;241m.\u001b[39mjson(), cast_to\u001b[38;5;241m=\u001b[39mcast_to, response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/llama_stack_client/_streaming.py:49\u001b[0m, in \u001b[0;36mStream._iter_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_events\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder\u001b[38;5;241m.\u001b[39miter_bytes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39miter_bytes())\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/llama_stack_client/_streaming.py:203\u001b[0m, in \u001b[0;36mSSEDecoder.iter_bytes\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miter_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: Iterator[\u001b[38;5;28mbytes\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_chunks(iterator):\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m raw_line \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39msplitlines():\n\u001b[1;32m    206\u001b[0m             line \u001b[38;5;241m=\u001b[39m raw_line\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/llama_stack_client/_streaming.py:214\u001b[0m, in \u001b[0;36mSSEDecoder._iter_chunks\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39msplitlines(keepends\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    216\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m line\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_models.py:897\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    898\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_models.py:951\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    948\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_client.py:153\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_transports/default.py:126\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/dev/llama-stack/envs/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: peer closed connection without sending complete message body (incomplete chunked read)"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    tools=[\n",
    "        {\n",
    "            \"tool_name\": \"get_flight_info\",\n",
    "            \"description\": \"Get the flight information for a given origin and destination\",\n",
    "            \"parameters\": {\n",
    "                \"origin\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The origin airport code. E.g., AU\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
