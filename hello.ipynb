{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(completion_message=CompletionMessage(content=\"Hello! It's great to meet a world client like you. How can I assist you today?\", role='assistant', stop_reason='end_of_turn', tool_calls=[]), logprobs=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from IPython.display import display\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:5001\",\n",
    ")\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world client!\"},\n",
    "    ],\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "display(response)\n",
    "\n",
    "# for chunk in response:\n",
    "    # print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inference': [ProviderInfo(provider_id='groq', provider_type='remote::groq')],\n",
       " 'memory': [ProviderInfo(provider_id='faiss', provider_type='inline::faiss')],\n",
       " 'safety': [ProviderInfo(provider_id='llama-guard', provider_type='inline::llama-guard')],\n",
       " 'agents': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'telemetry': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'eval': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'datasetio': [ProviderInfo(provider_id='localfs', provider_type='inline::localfs')],\n",
       " 'scoring': [ProviderInfo(provider_id='basic', provider_type='inline::basic')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.providers.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        # {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello World\"},\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a specification for building asynchronous web servers and applications in Python. It provides a standardized way for developers to write asynchronous code that can be executed by multiple frameworks and servers.\n",
      "\n",
      "Here's a high-level overview of how ASGI works:\n",
      "\n",
      "**ASGI Spec**\n",
      "\n",
      "The ASGI specification defines a few key components:\n",
      "\n",
      "1. **ASGI Application**: This is the core of the ASGI system. It's an object that implements the `asgi` module, which provides an interface for handling incoming requests and sending responses.\n",
      "2. **ASGI Server**: This is the component that runs the ASGI application. It's responsible for accepting incoming connections, dispatching requests to the application, and sending responses back to the client.\n",
      "3. **ASGI Protocol**: This is the communication protocol used by the ASGI server and application to exchange messages. It's based on TCP and supports bidirectional communication.\n",
      "\n",
      "**ASGI Request/Response Cycle**\n",
      "\n",
      "Here's a step-by-step breakdown of the ASGI request/response cycle:\n",
      "\n",
      "1. **Client connects**: A client (e.g., a web browser) establishes a connection to the ASGI server.\n",
      "2. **Server dispatches**: The server dispatches the incoming connection to the ASGI application.\n",
      "3. **Application receives**: The ASGI application receives the request from the server and performs any necessary processing (e.g., handling forms, authenticating users, or validating input).\n",
      "4. **Application sends response**: The ASGI application sends a response back to the server, which is then sent to the client.\n",
      "5. **Server sends response**: The server sends the response to the client.\n",
      "\n",
      "**ASGI Protocol Messages**\n",
      "\n",
      "The ASGI protocol uses the following types of messages to communicate between the server and application:\n",
      "\n",
      "1. ** message**: This is the primary message type used for request/response exchange. It contains the request data, such as headers, method, and path.\n",
      "2. **ready**: This message is used to indicate that the application is ready to receive the next message.\n",
      "3. **complete**: This message is used to indicate that the response is complete and can be sent to the client.\n",
      "\n",
      "**Python Framework Support**\n",
      "\n",
      "Several Python frameworks support ASGI, including:\n",
      "\n",
      "1. **ASGI**: The official ASGI specification provides a reference implementation in Python.\n",
      "2. **Django**: Django 3.1 and later versions support ASGI through the `django.asgi` module.\n",
      "3. **FastAPI**: FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "4. **Sanic**: Sanic is a Python 3.7+ web framework that's built on top of uvloop and httptools, allowing for high-performance async web development.\n",
      "\n",
      "When using ASGI with a Python framework, you typically need to:\n",
      "\n",
      "1. Create an ASGI application object, which implements the `asgi` module.\n",
      "2. Register the application with the ASGI server.\n",
      "3. Start the ASGI server to begin handling incoming requests and sending responses.\n",
      "\n",
      "By using ASGI, you can build high-performance, scalable, and concurrent web applications with Python."
     ]
    }
   ],
   "source": [
    "from llama_models.datatypes import SamplingParams\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a standard for Python web servers that allows them to communicate with web frameworks and other applications in an asynchronous manner. It's designed to be a replacement for the older WSGI (Web Server Gateway Interface) standard, which was synchronous.\n",
      "\n",
      "Here's a high-level overview of how ASGI works:\n",
      "\n",
      "**The Problem with WSGI**\n",
      "\n",
      "WSGI was introduced in the early 2000s as a way for web frameworks to communicate with web servers. However, WSGI is synchronous, meaning that it blocks the execution of the web server until the web framework has finished processing the request. This can lead to performance issues and scalability problems, especially in modern web applications that require handling many concurrent requests.\n",
      "\n",
      "**The Solution: ASGI**\n",
      "\n",
      "ASGI addresses the limitations of WSGI by introducing an asynchronous programming model. Instead of blocking the execution of the web server, ASGI allows the web framework to yield control back to the web server at specific points in the request processing pipeline. This allows the web server to handle other requests concurrently, improving performance and scalability.\n",
      "\n",
      "**The ASGI Protocol**\n",
      "\n",
      "The ASGI protocol defines a set of messages that can be sent between the web server and the web framework. These messages include:\n",
      "\n",
      "1. `connect`: The web server sends a `connect` message to the web framework to initiate a new connection.\n",
      "2. `receive`: The web framework sends a `receive` message to the web server to receive a request.\n",
      "3. `send`: The web framework sends a `send` message to the web server to send a response.\n",
      "4. `close`: The web framework sends a `close` message to the web server to close the connection.\n",
      "\n",
      "**How ASGI Works**\n",
      "\n",
      "Here's a step-by-step explanation of how ASGI works:\n",
      "\n",
      "1. The web server receives a request and sends a `connect` message to the web framework.\n",
      "2. The web framework processes the request and yields control back to the web server at specific points in the request processing pipeline (e.g., when waiting for I/O operations to complete).\n",
      "3. The web server handles other requests concurrently while the web framework is yielding control.\n",
      "4. When the web framework is ready to send a response, it sends a `send` message to the web server.\n",
      "5. The web server sends the response back to the client.\n",
      "6. When the response is complete, the web framework sends a `close` message to the web server to close the connection.\n",
      "\n",
      "**ASGI Implementations**\n",
      "\n",
      "There are several ASGI implementations available for Python, including:\n",
      "\n",
      "1. `uvicorn`: A popular ASGI server that supports HTTP/1.1 and HTTP/2.\n",
      "2. `hypercorn`: Another popular ASGI server that supports HTTP/1.1 and HTTP/2.\n",
      "3. `daphne`: An ASGI server that supports WebSockets and other advanced features.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "ASGI is a powerful standard for Python web servers that allows them to communicate with web frameworks in an asynchronous manner. By yielding control back to the web server at specific points in the request processing pipeline, ASGI enables web servers to handle many concurrent requests, improving performance and scalability."
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        top_p=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a standard for Python web servers that allows them to communicate with web frameworks and other applications in an asynchronous manner. It's designed to be a replacement for the older WSGI (Web Server Gateway Interface"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ToolDefinition' from 'llama_models.datatypes' (/Users/aidand/dev/llama-stack/envs/lib/python3.10/site-packages/llama_models/datatypes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatatypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolDefinition\n\u001b[1;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39minference\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m      4\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToolDefinition' from 'llama_models.datatypes' (/Users/aidand/dev/llama-stack/envs/lib/python3.10/site-packages/llama_models/datatypes.py)"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    "    logprobs={\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(completion_message=CompletionMessage(content='', role='assistant', stop_reason='end_of_message', tool_calls=[ToolCall(arguments={'origin': 'ADL', 'destination': 'SYD'}, call_id='call_77gr', tool_name='get_flight_info')]), logprobs=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "    ],\n",
    "    # stream=True,\n",
    "    tools=[\n",
    "        {\n",
    "            \"tool_name\": \"get_flight_info\",\n",
    "            \"description\": \"Get the flight information for a given origin and destination\",\n",
    "            \"parameters\": {\n",
    "                \"origin\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The origin airport code. E.g., AU\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk.event.delta, end='')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ChatCompletion(\n",
    "    id='chatcmpl-7f14606b-d091-4b12-9d13-e95831f04301',\n",
    "    choices=[\n",
    "        Choice(\n",
    "            finish_reason='tool_calls',\n",
    "            index=0,\n",
    "            logprobs=None,\n",
    "            message=ChatCompletionMessage(\n",
    "                content=None,\n",
    "                role='assistant',\n",
    "                function_call=None,\n",
    "                tool_calls=[ChatCompletionMessageToolCall(id='call_4qg1', function=Function(arguments='{\"origin\":\"ADL\",\"destination\":\"SYD\"}', name='get_flight_info'), type='function')]\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    created=1733917567,\n",
    "    model='llama3-8b-8192',\n",
    "    object='chat.completion',\n",
    "    system_fingerprint='fp_a97cfe35ae',\n",
    "    usage=CompletionUsage(completion_tokens=76, prompt_tokens=972, total_tokens=1048, completion_time=0.063333333, prompt_time=0.11611327, queue_time=0.0061331509999999895, total_time=0.179446603),\n",
    "    x_groq={'id': 'req_01jetrmtcmfs89v7qyw8fdx1v0'}\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionResponseStreamChunkEventDeltaToolCallDelta(content=ToolCall(arguments={'origin': 'ADL', 'destination': 'SYD'}, call_id='call_4d20', tool_name='get_flight_info'), parse_status='in_progress')"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    tools=[\n",
    "        {\n",
    "            \"tool_name\": \"get_flight_info\",\n",
    "            \"description\": \"Get the flight information for a given origin and destination\",\n",
    "            \"parameters\": {\n",
    "                \"origin\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The origin airport code. E.g., AU\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "                \"destination\": {\n",
    "                    \"param_type\": \"string\",\n",
    "                    \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "                    \"required\": True,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ChatCompletionChunk(\n",
    "    id='chatcmpl-189b0530-6bcb-4089-bad7-65f73104b182', \n",
    "    choices=[\n",
    "        Choice(\n",
    "            delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), \n",
    "            finish_reason=None, \n",
    "            index=0, \n",
    "            logprobs=None\n",
    "        )\n",
    "    ], \n",
    "    created=1733955177, \n",
    "    model='llama3-8b-8192', \n",
    "    object='chat.completion.chunk', \n",
    "    system_fingerprint='fp_a97cfe35ae', \n",
    "    usage=None, \n",
    "    x_groq=XGroq(id='req_01jevwgjx2f3maj4rbzaaexagx', usage=None, error=None))\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
