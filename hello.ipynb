{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(completion_message=CompletionMessage(content='Hello!', role='assistant', stop_reason='end_of_turn', tool_calls=[]), logprobs=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "from IPython.display import display\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:5001\",\n",
    ")\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world client!\"},\n",
    "    ],\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "display(response)\n",
    "\n",
    "# for chunk in response:\n",
    "    # print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inference': [ProviderInfo(provider_id='groq', provider_type='remote::groq')],\n",
       " 'memory': [ProviderInfo(provider_id='faiss', provider_type='inline::faiss')],\n",
       " 'safety': [ProviderInfo(provider_id='llama-guard', provider_type='inline::llama-guard')],\n",
       " 'agents': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'telemetry': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'eval': [ProviderInfo(provider_id='meta-reference', provider_type='inline::meta-reference')],\n",
       " 'datasetio': [ProviderInfo(provider_id='localfs', provider_type='inline::localfs')],\n",
       " 'scoring': [ProviderInfo(provider_id='basic', provider_type='inline::basic')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.providers.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        # {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello World\"},\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a specification for building asynchronous web servers and applications in Python. It provides a standard way for web frameworks to communicate with web servers, allowing for efficient and scalable handling of HTTP requests.\n",
      "\n",
      "Here's a high-level overview of how ASGI works:\n",
      "\n",
      "**The ASGI Spec**\n",
      "\n",
      "The ASGI spec defines a simple, text-based protocol for communication between a web server and a web application. It consists of three main components:\n",
      "\n",
      "1. **ASGI protocol**: A text-based protocol that defines the format of messages exchanged between the web server and the web application.\n",
      "2. **ASGI application**: A Python callable that implements the ASGI protocol and handles incoming HTTP requests.\n",
      "3. **ASGI server**: A Python module that implements the ASGI protocol and manages the communication between the web server and the ASGI application.\n",
      "\n",
      "**The ASGI Request-Response Cycle**\n",
      "\n",
      "Here's a step-by-step breakdown of the ASGI request-response cycle:\n",
      "\n",
      "1. **Web server receives request**: A web server (e.g., Nginx, Apache) receives an incoming HTTP request.\n",
      "2. **ASGI server receives request**: The ASGI server receives the request from the web server and converts it into an ASGI message.\n",
      "3. **ASGI application handles request**: The ASGI application receives the ASGI message and processes the request. This may involve executing Python code, interacting with databases, or calling other services.\n",
      "4. **ASGI application sends response**: The ASGI application sends a response back to the ASGI server, which is converted into an HTTP response.\n",
      "5. **ASGI server sends response**: The ASGI server sends the HTTP response back to the web server.\n",
      "6. **Web server sends response**: The web server sends the HTTP response to the client (e.g., a web browser).\n",
      "\n",
      "**ASGI Messages**\n",
      "\n",
      "ASGI messages are the core of the ASGI protocol. They are text-based messages that contain information about the request or response, such as:\n",
      "\n",
      "* `type`: The type of message (e.g., `http.request`, `http.response`).\n",
      "* `body`: The request or response body (e.g., JSON data).\n",
      "* `headers`: The request or response headers (e.g., `Content-Type`, `Authorization`).\n",
      "\n",
      "Here's an example of an ASGI message:\n",
      "```python\n",
      "{\n",
      "    \"type\": \"http.request\",\n",
      "    \"body\": \"{\\\"name\\\":\\\"John\\\"}\",\n",
      "    \"headers\": {\n",
      "        \"Content-Type\": \"application/json\",\n",
      "        \"Host\": \"example.com\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "**ASGI Frameworks**\n",
      "\n",
      "Several ASGI frameworks are available for Python, including:\n",
      "\n",
      "* **ASGI**: The official ASGI framework, which provides a simple and lightweight way to build ASGI applications.\n",
      "* **Starlette**: A popular ASGI framework that provides a robust set of features for building web applications.\n",
      "* **FastAPI**: A modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "\n",
      "These frameworks provide a higher-level abstraction for building ASGI applications, making it easier to develop and deploy web applications.\n",
      "\n",
      "I hope this helps you understand the basics of ASGI in Python!"
     ]
    }
   ],
   "source": [
    "from llama_models.datatypes import SamplingParams\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a standard for Python web servers that allows them to communicate with web frameworks and other applications in an asynchronous manner. It's designed to be a replacement for the older WSGI (Web Server Gateway Interface) standard, which was synchronous.\n",
      "\n",
      "Here's a high-level overview of how ASGI works:\n",
      "\n",
      "**The Problem with WSGI**\n",
      "\n",
      "WSGI was introduced in the early 2000s as a way for web frameworks to communicate with web servers. However, WSGI is synchronous, meaning that it blocks the execution of the web server until the web framework has finished processing the request. This can lead to performance issues and scalability problems, especially in modern web applications that require handling many concurrent requests.\n",
      "\n",
      "**The Solution: ASGI**\n",
      "\n",
      "ASGI addresses the limitations of WSGI by introducing an asynchronous programming model. Instead of blocking the execution of the web server, ASGI allows the web framework to yield control back to the web server at specific points in the request processing pipeline. This allows the web server to handle other requests concurrently, improving performance and scalability.\n",
      "\n",
      "**The ASGI Protocol**\n",
      "\n",
      "The ASGI protocol defines a set of messages that can be sent between the web server and the web framework. These messages include:\n",
      "\n",
      "1. `connect`: The web server sends a `connect` message to the web framework to initiate a new connection.\n",
      "2. `receive`: The web framework sends a `receive` message to the web server to receive a request.\n",
      "3. `send`: The web framework sends a `send` message to the web server to send a response.\n",
      "4. `close`: The web framework sends a `close` message to the web server to close the connection.\n",
      "\n",
      "**How ASGI Works**\n",
      "\n",
      "Here's a step-by-step explanation of how ASGI works:\n",
      "\n",
      "1. The web server receives a request and sends a `connect` message to the web framework.\n",
      "2. The web framework processes the request and yields control back to the web server at specific points in the request processing pipeline (e.g., when waiting for I/O operations to complete).\n",
      "3. The web server handles other requests concurrently while the web framework is yielding control.\n",
      "4. When the web framework is ready to send a response, it sends a `send` message to the web server.\n",
      "5. The web server sends the response back to the client.\n",
      "6. When the response is complete, the web framework sends a `close` message to the web server to close the connection.\n",
      "\n",
      "**ASGI Implementations**\n",
      "\n",
      "There are several ASGI implementations available for Python, including:\n",
      "\n",
      "1. `uvicorn`: A popular ASGI server that supports HTTP/1.1 and HTTP/2.\n",
      "2. `hypercorn`: Another popular ASGI server that supports HTTP/1.1 and HTTP/2.\n",
      "3. `daphne`: An ASGI server that supports WebSockets and other advanced features.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "ASGI is a powerful standard for Python web servers that allows them to communicate with web frameworks in an asynchronous manner. By yielding control back to the web server at specific points in the request processing pipeline, ASGI enables web servers to handle many concurrent requests, improving performance and scalability."
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        top_p=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a specification for building asynchronous web servers and applications in Python. It's similar to the WSGI (Web Server Gateway Interface) specification, but designed for use with asynchronous code.\n",
      "\n",
      "Here's a high"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGI (Asynchronous Server Gateway Interface) is a standard for Python web servers that allows them to communicate with web frameworks and other applications in an asynchronous manner. It's designed to be a replacement for the older WSGI (Web Server Gateway Interface"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    model_id=\"Llama3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain to me how ASGI in python works\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=50\n",
    "    ),\n",
    "    logprobs={\n",
    "        \"top_k\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.event.delta, end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.inference.chat_completion(\n",
    "#     model_id=\"Llama3.2-3B-Instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "#     ],\n",
    "#     # stream=True,\n",
    "#     tools=[\n",
    "#         {\n",
    "#             \"tool_name\": \"get_flight_info\",\n",
    "#             \"description\": \"Get the flight information for a given origin and destination\",\n",
    "#             \"parameters\": {\n",
    "#                 \"origin\": {\n",
    "#                     \"param_type\": \"string\",\n",
    "#                     \"description\": \"The origin airport code. E.g., AU\",\n",
    "#                     \"required\": True,\n",
    "#                 },\n",
    "#                 \"destination\": {\n",
    "#                     \"param_type\": \"string\",\n",
    "#                     \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "#                     \"required\": True,\n",
    "#                 },\n",
    "#             }\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # for chunk in response:\n",
    "# #     print(chunk.event.delta, end='')\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ChatCompletion(\n",
    "    id='chatcmpl-7f14606b-d091-4b12-9d13-e95831f04301',\n",
    "    choices=[\n",
    "        Choice(\n",
    "            finish_reason='tool_calls',\n",
    "            index=0,\n",
    "            logprobs=None,\n",
    "            message=ChatCompletionMessage(\n",
    "                content=None,\n",
    "                role='assistant',\n",
    "                function_call=None,\n",
    "                tool_calls=[ChatCompletionMessageToolCall(id='call_4qg1', function=Function(arguments='{\"origin\":\"ADL\",\"destination\":\"SYD\"}', name='get_flight_info'), type='function')]\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    created=1733917567,\n",
    "    model='llama3-8b-8192',\n",
    "    object='chat.completion',\n",
    "    system_fingerprint='fp_a97cfe35ae',\n",
    "    usage=CompletionUsage(completion_tokens=76, prompt_tokens=972, total_tokens=1048, completion_time=0.063333333, prompt_time=0.11611327, queue_time=0.0061331509999999895, total_time=0.179446603),\n",
    "    x_groq={'id': 'req_01jetrmtcmfs89v7qyw8fdx1v0'}\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = client.inference.chat_completion(\n",
    "#     model_id=\"Llama3.2-3B-Instruct\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"When's the next flight from Adelaide to Sydney?\"},\n",
    "#     ],\n",
    "#     stream=True,\n",
    "#     tools=[\n",
    "#         {\n",
    "#             \"tool_name\": \"get_flight_info\",\n",
    "#             \"description\": \"Get the flight information for a given origin and destination\",\n",
    "#             \"parameters\": {\n",
    "#                 \"origin\": {\n",
    "#                     \"param_type\": \"string\",\n",
    "#                     \"description\": \"The origin airport code. E.g., AU\",\n",
    "#                     \"required\": True,\n",
    "#                 },\n",
    "#                 \"destination\": {\n",
    "#                     \"param_type\": \"string\",\n",
    "#                     \"description\": \"The destination airport code. E.g., 'LAX'\",\n",
    "#                     \"required\": True,\n",
    "#                 },\n",
    "#             }\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# for chunk in response:\n",
    "#     print(chunk.event.delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ChatCompletionChunk(\n",
    "    id='chatcmpl-189b0530-6bcb-4089-bad7-65f73104b182', \n",
    "    choices=[\n",
    "        Choice(\n",
    "            delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=None), \n",
    "            finish_reason=None, \n",
    "            index=0, \n",
    "            logprobs=None\n",
    "        )\n",
    "    ], \n",
    "    created=1733955177, \n",
    "    model='llama3-8b-8192', \n",
    "    object='chat.completion.chunk', \n",
    "    system_fingerprint='fp_a97cfe35ae', \n",
    "    usage=None, \n",
    "    x_groq=XGroq(id='req_01jevwgjx2f3maj4rbzaaexagx', usage=None, error=None))\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
