export OLLAMA_INFERENCE_MODEL="llama3.2:3b-instruct-fp16"
export LLAMA_STACK_PORT=5001
export INFERENCE_MODEL=meta-llama/Llama-3.2-11B-Vision-Instruct
export INFERENCE_PORT=8000
export VLLM_URL=http://localhost:8000/v1
export SAFETY_MODEL=meta-llama/Llama-Guard-3-1B
export SQLITE_STORE_DIR=$LLAMA_STACK_CONFIG_DIR/distributions/meta-reference-gpu
